{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sentiment_Analysis_Pyspark.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OYqwcoLbRQcR",
        "colab_type": "text"
      },
      "source": [
        "Configured Spark Environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MOVsCq4qNQd7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 415
        },
        "outputId": "9ff01b8d-40dc-4321-8831-df51f346944e"
      },
      "source": [
        "!apt-get -y install openjdk-8-jre-headless\n",
        "!pip install pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark import SparkContext\n",
        "spark = SparkSession.builder.master(\"local\").getOrCreate()\n",
        "sc = SparkContext.getOrCreate()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-440\n",
            "Use 'apt autoremove' to remove it.\n",
            "Suggested packages:\n",
            "  libnss-mdns fonts-dejavu-extra fonts-ipafont-gothic fonts-ipafont-mincho\n",
            "  fonts-wqy-microhei fonts-wqy-zenhei fonts-indic\n",
            "The following NEW packages will be installed:\n",
            "  openjdk-8-jre-headless\n",
            "0 upgraded, 1 newly installed, 0 to remove and 35 not upgraded.\n",
            "Need to get 27.5 MB of archives.\n",
            "After this operation, 101 MB of additional disk space will be used.\n",
            "Ign:1 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 openjdk-8-jre-headless amd64 8u252-b09-1~18.04\n",
            "Err:1 http://security.ubuntu.com/ubuntu bionic-updates/universe amd64 openjdk-8-jre-headless amd64 8u252-b09-1~18.04\n",
            "  404  Not Found [IP: 91.189.88.142 80]\n",
            "E: Failed to fetch http://security.ubuntu.com/ubuntu/pool/universe/o/openjdk-8/openjdk-8-jre-headless_8u252-b09-1~18.04_amd64.deb  404  Not Found [IP: 91.189.88.142 80]\n",
            "E: Unable to fetch some archives, maybe run apt-get update or try with --fix-missing?\n",
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.6/dist-packages (3.0.0)\n",
            "Requirement already satisfied: py4j==0.10.9 in /usr/local/lib/python3.6/dist-packages (from pyspark) (0.10.9)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JM8vuQ0aPcAI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pyspark.sql.types\n",
        "import pyspark\n",
        "from pyspark.sql.types import StructField\n",
        "from pyspark.sql.types import StructType,IntegerType,StringType\n",
        "Schema = StructType([StructField('id', IntegerType(), True),\n",
        "              StructField('text', StringType(), True),\n",
        "              StructField('target', IntegerType(), True)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fgi0VEWfQJ-k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df1 = spark.read.format(\"csv\").option(\"encoding\",\"gbk\").option(\"header\",True).load(r\"clean_tweet1.csv\", schema=Schema) \n",
        "df2 = spark.read.format(\"csv\").option(\"encoding\",\"gbk\").option(\"header\",True).load(r\"clean_tweet2.csv\", schema=Schema)\n",
        "df3 = spark.read.format(\"csv\").option(\"encoding\",\"gbk\").option(\"header\",True).load(r\"clean_tweet3.csv\", schema=Schema) \n",
        "df4 = spark.read.format(\"csv\").option(\"encoding\",\"gbk\").option(\"header\",True).load(r\"clean_tweet4.csv\", schema=Schema)  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Y0mKfg3QLyY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 665
        },
        "outputId": "14a63f1f-c42c-4b61-90bd-5c18dddbe61a"
      },
      "source": [
        "df1.show(3)\n",
        "df2.show(3)\n",
        "df3.show(3)\n",
        "df4.show(3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+--------------------+------+\n",
            "| id|                text|target|\n",
            "+---+--------------------+------+\n",
            "|  0|awww bummer shoul...|     0|\n",
            "|  1|upset can not upd...|     0|\n",
            "|  2|dive mani time ba...|     0|\n",
            "+---+--------------------+------+\n",
            "only showing top 3 rows\n",
            "\n",
            "+------+--------------------+------+\n",
            "|    id|                text|target|\n",
            "+------+--------------------+------+\n",
            "|400000|    mutha effin bore|     0|\n",
            "|400001|not spent afterno...|     0|\n",
            "|400002|My entir friggin ...|     0|\n",
            "+------+--------------------+------+\n",
            "only showing top 3 rows\n",
            "\n",
            "+------+--------------------+------+\n",
            "|    id|                text|target|\n",
            "+------+--------------------+------+\n",
            "|800000|       love guy best|     4|\n",
            "|800001|im meet one besti...|     4|\n",
            "|800002|thank twitter add...|     4|\n",
            "+------+--------------------+------+\n",
            "only showing top 3 rows\n",
            "\n",
            "+-------+--------------------+------+\n",
            "|     id|                text|target|\n",
            "+-------+--------------------+------+\n",
            "|1200000|daniel best thing...|     4|\n",
            "|1200001|need new hobbi wa...|     4|\n",
            "|1200002|looov friend toda...|     4|\n",
            "+-------+--------------------+------+\n",
            "only showing top 3 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mss5cPDeQRLM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# merge \n",
        "data = df1.union(df2)\n",
        "data = data.union(df3)\n",
        "data = data.union(df4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mOVyEvtBojGs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "a79ecf34-2923-401c-a360-dd364cde3bdf"
      },
      "source": [
        "data.count()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1599996"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oitvlrvlok6N",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "c24b8fc5-5b4e-4565-c7ce-ccafb91fd8b7"
      },
      "source": [
        "data = data.drop('id')\n",
        "data = data.dropna()\n",
        "data.count()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1593372"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gWoLPDrJpF-x",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "0e6c7107-d379-4214-d096-0126171130e4"
      },
      "source": [
        "(train_set, test_set) = data.randomSplit([0.8, 0.2], seed = 2000)\n",
        "print(\"Train set has total {0} entries with {1:.2f}% negative, {2:.2f}% positive\".format(train_set.count(),(train_set[train_set['target'] == 0].count() / ((train_set.count())*1.))*100, (train_set[train_set['target'] == 4].count()/((train_set.count())*1.))*100))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train set has total 1274792 entries with 50.05% negative, 49.95% positive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dOpocQnuKyt6",
        "colab_type": "text"
      },
      "source": [
        "Define Different Pipeline  \n",
        "\n",
        "1. Compare different features  \n",
        "\n",
        "\n",
        "*   TFIDF Vectorizer *vs*  Count Vectorizer\n",
        "*   1-gram *vs* 2-gram *vs* 3-gram *vs* combination\n",
        "\n",
        "2. Compare different models\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hkOvRSZ-qTH3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
        "from pyspark.ml.feature import StringIndexer\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.feature import CountVectorizer\n",
        "\n",
        "# setup the pipeline\n",
        "# stage: transform the category to numeric\n",
        "label_stringIdx = StringIndexer(inputCol = \"target\", outputCol = \"label\")\n",
        "def tweet_pipeline(feature_stages,model,train_set,test_set):\n",
        "  pipeline_stages = [label_stringIdx] + feature_stages + [model]\n",
        "  pipeline = Pipeline(stages= pipeline_stages)\n",
        "  # # fit the pipeline for the trainind data\n",
        "  model = pipeline.fit(train_set)\n",
        "  # # transform the data\n",
        "  # # transform_train = model.transform(train_set)\n",
        "  prediction = model.transform(test_set)\n",
        "\n",
        "  return prediction\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VCamqLogSGXN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.ml.classification import RandomForestClassifier\n",
        "\n",
        "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
        "cv = CountVectorizer(vocabSize=2**16, inputCol=\"words\", outputCol='cv')\n",
        "cv_idf = IDF(inputCol='cv', outputCol=\"features\", minDocFreq=5) #minDocFreq: remove sparse terms\n",
        "hashtf = HashingTF(numFeatures=2**16, inputCol=\"words\", outputCol='tf')\n",
        "tf_idf = IDF(inputCol='tf', outputCol=\"features\", minDocFreq=5) #minDocFreq: remove sparse terms\n",
        "\n",
        "# rl= LogisticRegression(featuresCol='features',labelCol='label'ï¼ŒmaxIter=100)\n",
        "lr = LogisticRegression(maxIter=100)\n",
        "rf = RandomForestClassifier(numTrees=10)\n",
        "\n",
        "\n",
        "lr_cvidf_prediction = tweet_pipeline([tokenizer, cv, cv_idf],lr,train_set,test_set)\n",
        "lr_tfidf_prediction = tweet_pipeline([tokenizer, hashtf, tf_idf],lr,train_set,test_set)\n",
        "# rf_cvidf_prediction = tweet_pipeline([tokenizer, cv, cv_idf],rf,train_set,test_set)\n",
        "# rf_tfidf_prediction = tweet_pipeline([tokenizer, hashtf, tf_idf],rf,train_set,test_set)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eQm4RMouzlj1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "a1e529a0-473e-4eea-9144-254e7f3d4d5b"
      },
      "source": [
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\")\n",
        "evaluator.evaluate(lr_cvidf_prediction)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8552051787564793"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LUKlDUWtTcbJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "8465b076-77cd-47b4-bbe0-7f2a231037b6"
      },
      "source": [
        "evaluator.evaluate(lr_tfidf_prediction)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8493910477487672"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d9sF53_Z02tS",
        "colab_type": "text"
      },
      "source": [
        "Feature Extaction  \n",
        "1. Count Vectorizer (unigram/ bigram/ trigram)\n",
        "2. TFIDF Vectorizer (unigram/ bigram/ trigram)\n",
        "3. Word2Vec"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "58B8y1ff014L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyspark.ml.feature import NGram, VectorAssembler\n",
        "def build_ngrams_wocs(inputCol=[\"text\",\"target\"], n=3):\n",
        "    tokenizer = [Tokenizer(inputCol=\"text\", outputCol=\"words\")]\n",
        "    ngrams = [\n",
        "        NGram(n=i, inputCol=\"words\", outputCol=\"{0}_grams\".format(i))\n",
        "        for i in range(1, n + 1)\n",
        "    ]\n",
        "\n",
        "    cv = [\n",
        "        CountVectorizer(vocabSize=5460,inputCol=\"{0}_grams\".format(i),\n",
        "            outputCol=\"{0}_tf\".format(i))\n",
        "        for i in range(1, n + 1)\n",
        "    ]\n",
        "    idf = [IDF(inputCol=\"{0}_tf\".format(i), outputCol=\"{0}_tfidf\".format(i), minDocFreq=5) for i in range(1, n + 1)]\n",
        "\n",
        "    assembler = [VectorAssembler(\n",
        "        inputCols=[\"{0}_tfidf\".format(i) for i in range(1, n + 1)],\n",
        "        outputCol=\"features\"\n",
        "    )]\n",
        "    label_stringIdx = [StringIndexer(inputCol = \"target\", outputCol = \"label\")]\n",
        "    lr = [LogisticRegression(maxIter=100)]\n",
        "    return Pipeline(stages=tokenizer + ngrams + cv + idf+ assembler + label_stringIdx+lr)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fZ-5QJbciPEy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer = [Tokenizer(inputCol=\"text\", outputCol=\"words\")]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V9MuxinccQ5a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyspark.ml.feature import NGram, VectorAssembler\n",
        "n = 3\n",
        "ngrams = [NGram(n=i, inputCol=\"words\", outputCol=\"{0}_grams\".format(i))\n",
        "        for i in range(1, n + 1)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XvqT7o2pcqZf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cv = [CountVectorizer(vocabSize=5460,inputCol=\"{0}_grams\".format(i),\n",
        "            outputCol=\"{0}_tf\".format(i))\n",
        "        for i in range(1, n + 1)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rxf8CcYJhcG6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "idf = [IDF(inputCol=\"{0}_tf\".format(i), outputCol=\"{0}_tfidf\".format(i), minDocFreq=5) for i in range(1, n + 1)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eiGXheWohoXm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "assembler = [VectorAssembler(\n",
        "        inputCols=[\"{0}_tfidf\".format(i) for i in range(1, n + 1)],\n",
        "        outputCol=\"features\"\n",
        "    )]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iYRVy6qChqmU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "from pyspark.ml.classification import NaiveBayes\n",
        "label_stringIdx = [StringIndexer(inputCol = \"target\", outputCol = \"label\")]\n",
        "lr = [LogisticRegression(maxIter=100)]\n",
        "nb = [NaiveBayes(smoothing=1.0, modelType=\"bernoulli\")]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DxxLhsUCh1vN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trigramwocs_pipelineFit = Pipeline(stages=tokenizer + ngrams + cv + idf+ assembler + label_stringIdx+lr).fit(train_set)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iuPNtuhwiImH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictions_wocs = trigramwocs_pipelineFit.transform(test_set)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mjk-jgmgmGWJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "1c187bae-38a6-4039-81dd-5e1eae39070a"
      },
      "source": [
        "evaluator.evaluate(predictions_wocs)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8731309130073437"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6RDbItPimStw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# save trained model\n",
        "from pyspark.ml import PipelineModel\n",
        "\n",
        "modelPath = 'LogisticPipelineModel_3_gram'\n",
        "trigramwocs_pipelineFit.write().overwrite().save(modelPath)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oOGZBMYj1g38",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}